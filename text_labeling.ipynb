{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import openai\n",
    "\n",
    "# except ImportError:\n",
    "#     install_the_module(\"foo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #2. Create a new experiment\n",
    "# mlflow.set_experiment('My experiment')\n",
    "\n",
    "# #3. Create a new run - Quickstart\n",
    "# with mlflow.start_run():\n",
    "#     #Everything that happens here will be logged to the run, including the parameters and metrics below\n",
    "#     mlflow.log_param(\"param1\", 4)\n",
    "#     mlflow.log_metric(\"foo\", 1)\n",
    "#     mlflow.log_metric(\"foo\", 2)\n",
    "#     mlflow.log_metric(\"foo\", 3)\n",
    "#     time.sleep(2)\n",
    "#     mlflow.log_metric(\"foo\", 7)\n",
    "#     #In the real world, we obviously would like to log more than one metric, and we would like to log them at different times and storing the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.set_experiment('My first machine learning experiment22')\n",
    "\n",
    "# with mlflow.start_run():\n",
    "#     diabetes = load_diabetes()\n",
    "#     X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names) #X is a pandas DataFrame containing the features\n",
    "#     y = diabetes.target #y is a numpy array containing the target variable\n",
    "\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) #The dataset is split into training and test sets\n",
    "\n",
    "#     #Set the number of trees you want to use with the n_estimators parameter and log it with mlflow.log_param()\n",
    "#     n_estimators = 100\n",
    "    \n",
    "\n",
    "#     #train an XGB model\n",
    "#     xgb_model = XGBRegressor(n_estimators=n_estimators).fit(X_train, y_train)\n",
    "\n",
    "#     #evaluate the model and log it into mlflow\n",
    "\n",
    "#     preds = xgb_model.predict(X_test)\n",
    "#     rmse = mean_squared_error(y_test, preds)**0.5\n",
    "#     mlflow.log_metric('rmse', rmse)\n",
    "\n",
    "#     with open('model.pkl', 'wb') as f:\n",
    "#         pickle.dump(xgb_model, f)\n",
    "#         mlflow.log_artifact('model.pkl') #log the model as an artifact\n",
    "#         #the artifact itself can be anything! a model, a dataset, a file, etc.\n",
    "#     os.remove('model.pkl')\n",
    "\n",
    "#     #log everything needed\n",
    "#     mlflow.log_params({'n_estimators': n_estimators}) #a dictionary of parameters\n",
    "#     mlflow.log_metrics({'rmse': rmse}) #a dictionary of metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example dataset\n",
    "# texts = [\n",
    "#     \"This is a positive review.\",\n",
    "#     \"Negative sentiment detected in this text.\",\n",
    "#     \"Another positive example.\",\n",
    "#     # Add more text samples\n",
    "# ]\n",
    "\n",
    "# labels = [1, 0, 1,  # 1 for positive, 0 for negative\n",
    "#           # Add corresponding labels for more samples\n",
    "#          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_texts = model.encode(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(encoded_texts, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegression()\n",
    "# clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# report = classification_report(y_test, y_pred)\n",
    "\n",
    "# print(f\"Accuracy: {accuracy}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_text = [\"This is a new negative.\", \"Another positive of new text.\"]\n",
    "\n",
    "# encoded_new_text = model.encode(new_text)\n",
    "\n",
    "# predicted_labels = clf.predict(encoded_new_text)\n",
    "\n",
    "# predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           label                                               text\n",
      "0       business  Lufthansa flies back to profit\\n\\nGerman airli...\n",
      "1       business  Winn-Dixie files for bankruptcy\\n\\nUS supermar...\n",
      "2       business  US economy still growing says Fed\\n\\nMost area...\n",
      "3       business  Saab to build Cadillacs in Sweden\\n\\nGeneral M...\n",
      "4       business  Bank voted 8-1 for no rate change\\n\\nThe decis...\n",
      "..           ...                                                ...\n",
      "995  technologie  Mobile games come of age\\n\\nThe BBC News websi...\n",
      "996  technologie  California sets fines for spyware\\n\\nThe maker...\n",
      "997  technologie  Web helps collect aid donations\\n\\nThe web is ...\n",
      "998  technologie  Mobiles rack up 20 years of use\\n\\nMobile phon...\n",
      "999  technologie  Blogs take on the mainstream\\n\\nWeb logs or bl...\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "data_folder_path = 'data'  # Update this to your actual folder path\n",
    "\n",
    "data_list = []\n",
    "folder_names = []\n",
    "\n",
    "for folder_name in os.listdir(data_folder_path):\n",
    "    folder_path = os.path.join(data_folder_path, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        folder_names.extend([folder_name] * len(os.listdir(folder_path)))  # Add folder names\n",
    "         \n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = folder_path+'/'+file_name\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                file_data = file.read()\n",
    "                data_list.append(file_data)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'label': folder_names, 'text': data_list})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "encoded_texts = model.encode(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.965\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       1.00      1.00      1.00        20\n",
      "entertainment       0.93      1.00      0.96        13\n",
      "         food       1.00      0.96      0.98        27\n",
      "     graphics       0.88      1.00      0.93        21\n",
      "   historical       1.00      1.00      1.00        15\n",
      "      medical       0.95      0.86      0.90        22\n",
      "     politics       0.96      1.00      0.98        25\n",
      "        space       0.93      1.00      0.96        13\n",
      "        sport       1.00      0.96      0.98        23\n",
      "  technologie       1.00      0.90      0.95        21\n",
      "\n",
      "     accuracy                           0.96       200\n",
      "    macro avg       0.96      0.97      0.97       200\n",
      " weighted avg       0.97      0.96      0.96       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(encoded_texts, df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['medical', 'space'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_text = [\"Medicine is a field of constant innovation and discovery, aimed at improving human health and well-being. From the development of life-saving vaccines to breakthroughs in surgical techniques, medical science has come a long way. In recent times, the world has witnessed an unprecedented global effort to combat diseases, with a spotlight on pandemics like COVID-19. Healthcare professionals, including doctors, nurses, and researchers, work tirelessly to provide the best care and find new treatments. Telemedicine has also gained prominence, allowing remote consultations and expanding access to healthcare. As the medical field continues to evolve, it offers hope for healthier lives and a brighter future.\", \"sport\"]\n",
    "encoded_new_text = model.encode(new_text)\n",
    "\n",
    "predicted_labels = clf.predict(encoded_new_text)\n",
    "\n",
    "sdfsdf\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\final_project\\text labeling.ipynb Cell 15\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m subject \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWrite a summary of climate change impacts on the environment.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Configure GPT-3 parameters\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     engine\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtext-davinci-003\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# You can choose other engines like 'text-davinci-003'\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     prompt\u001b[39m=\u001b[39;49msubject,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     max_tokens\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,  \u001b[39m# Adjust the desired length of the generated text\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     api_key\u001b[39m=\u001b[39;49mapi_key\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Extract the generated text from the response\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/final_project/text%20labeling.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m generated_text \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\python\\final_project\\venv\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\python\\final_project\\venv\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[1;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[0;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\python\\final_project\\venv\\lib\\site-packages\\openai\\api_requestor.py:298\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    289\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    290\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    297\u001b[0m     )\n\u001b[1;32m--> 298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\python\\final_project\\venv\\lib\\site-packages\\openai\\api_requestor.py:700\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    693\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    694\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    695\u001b[0m         )\n\u001b[0;32m    696\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    697\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    698\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 700\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    701\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    702\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    703\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    704\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    705\u001b[0m         ),\n\u001b[0;32m    706\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    707\u001b[0m     )\n",
      "File \u001b[1;32mc:\\python\\final_project\\venv\\lib\\site-packages\\openai\\api_requestor.py:765\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    763\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    764\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 765\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    766\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    767\u001b[0m     )\n\u001b[0;32m    768\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "# Replace 'your-api-key' with your actual GPT-3 API key\n",
    "api_key = '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'\n",
    "\n",
    "# Set the subject or prompt for text generation\n",
    "subject = \"Write a summary of climate change impacts on the environment.\"\n",
    "\n",
    "# Configure GPT-3 parameters\n",
    "response = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",  # You can choose other engines like 'text-davinci-003'\n",
    "    prompt=subject,\n",
    "    max_tokens=20,  # Adjust the desired length of the generated text\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Extract the generated text from the response\n",
    "generated_text = response.choices[0].text\n",
    "\n",
    "# Print the generated text\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
